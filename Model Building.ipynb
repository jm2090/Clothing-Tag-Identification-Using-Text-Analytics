{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import keras\n",
    "import spacy\n",
    "import functools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers import Flatten, Masking\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from random import randint\n",
    "from numpy import array, argmax, asarray, zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn import linear_model\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading 4 Data Frames from part 1\n",
    "df_color = pd.read_csv(\"color.csv\",index_col = 0)\n",
    "df_fit = pd.read_csv(\"fit.csv\",index_col = 0)\n",
    "df_occasion = pd.read_csv(\"occasion.csv\",index_col = 0)\n",
    "df_style = pd.read_csv(\"style.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming T/F to binary (we start getting labels from column 6 onwards that is why we just choose those to convert them to 1 and 0)\n",
    "df_color.iloc[:,6:]= df_color.iloc[:,6:].astype(int)\n",
    "df_fit.iloc[:,6:]= df_fit.iloc[:,6:].astype(int)\n",
    "df_occasion.iloc[:,6:]= df_occasion.iloc[:,6:].astype(int)\n",
    "df_style.iloc[:,6:]= df_style.iloc[:,6:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_full_name</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>details</th>\n",
       "      <th>blacks</th>\n",
       "      <th>pinks</th>\n",
       "      <th>whites</th>\n",
       "      <th>reds</th>\n",
       "      <th>...</th>\n",
       "      <th>grays</th>\n",
       "      <th>golds</th>\n",
       "      <th>navy</th>\n",
       "      <th>yellows</th>\n",
       "      <th>burgundies</th>\n",
       "      <th>purples</th>\n",
       "      <th>browns</th>\n",
       "      <th>multi</th>\n",
       "      <th>oranges</th>\n",
       "      <th>teal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DSECZPAGJJC1EDC79JRBF4WK</td>\n",
       "      <td>Banana Republic</td>\n",
       "      <td>Mock-Neck Sweater Top</td>\n",
       "      <td>Designed worn high-waisted bottom oh-so-now mo...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Designed worn high-waisted bottom oh-so-now mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DVA59VHYAPT4PVX32NXW91G5</td>\n",
       "      <td>Tibi</td>\n",
       "      <td>Juan Embossed Mules</td>\n",
       "      <td>Tibis Juan embossed mule made shiny black leat...</td>\n",
       "      <td>women:SHOES:MULES</td>\n",
       "      <td>seen Pre-Fall ‘Number runway Heel measure appr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01DVA4XY7A0QMMSK3V3SBR52J9</td>\n",
       "      <td>Alexandre Birman</td>\n",
       "      <td>Clarita Bow-Embellished Suede Sandals</td>\n",
       "      <td>Alexandre Birmans Clarita sandal quickly risen...</td>\n",
       "      <td>women:SHOES:SANDALS</td>\n",
       "      <td>Heel height measure approximately 50mm Number ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01DVBP9AHVQTZXJSBNJ0N2NYJP</td>\n",
       "      <td>Khaite</td>\n",
       "      <td>Leather ankle boots</td>\n",
       "      <td>Heel measure approximately 50mm Number inch Bl...</td>\n",
       "      <td>Shoes   Boots   Ankle</td>\n",
       "      <td>Fits true size take normal size Italian sizing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01DVBR93Y7KANZE3C09YCTVXDF</td>\n",
       "      <td>Lauren Manoogian</td>\n",
       "      <td>Alpaca-blend scarf</td>\n",
       "      <td>Brown alpaca-blend Number alpaca Number polyam...</td>\n",
       "      <td>Accessories   Scarves   Scarves</td>\n",
       "      <td>item measurement are Length 136cm Width 32cm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id             brand  \\\n",
       "0  01DSECZPAGJJC1EDC79JRBF4WK   Banana Republic   \n",
       "1  01DVA59VHYAPT4PVX32NXW91G5              Tibi   \n",
       "2  01DVA4XY7A0QMMSK3V3SBR52J9  Alexandre Birman   \n",
       "3  01DVBP9AHVQTZXJSBNJ0N2NYJP            Khaite   \n",
       "4  01DVBR93Y7KANZE3C09YCTVXDF  Lauren Manoogian   \n",
       "\n",
       "                       product_full_name  \\\n",
       "0                  Mock-Neck Sweater Top   \n",
       "1                    Juan Embossed Mules   \n",
       "2  Clarita Bow-Embellished Suede Sandals   \n",
       "3                    Leather ankle boots   \n",
       "4                     Alpaca-blend scarf   \n",
       "\n",
       "                                         description  \\\n",
       "0  Designed worn high-waisted bottom oh-so-now mo...   \n",
       "1  Tibis Juan embossed mule made shiny black leat...   \n",
       "2  Alexandre Birmans Clarita sandal quickly risen...   \n",
       "3  Heel measure approximately 50mm Number inch Bl...   \n",
       "4  Brown alpaca-blend Number alpaca Number polyam...   \n",
       "\n",
       "                    brand_category  \\\n",
       "0                          Unknown   \n",
       "1                women:SHOES:MULES   \n",
       "2              women:SHOES:SANDALS   \n",
       "3            Shoes   Boots   Ankle   \n",
       "4  Accessories   Scarves   Scarves   \n",
       "\n",
       "                                             details  blacks  pinks  whites  \\\n",
       "0  Designed worn high-waisted bottom oh-so-now mo...       1      0       1   \n",
       "1  seen Pre-Fall ‘Number runway Heel measure appr...       1      0       0   \n",
       "2  Heel height measure approximately 50mm Number ...       0      0       0   \n",
       "3     Fits true size take normal size Italian sizing       1      0       0   \n",
       "4       item measurement are Length 136cm Width 32cm       0      0       0   \n",
       "\n",
       "   reds  ...  grays  golds  navy  yellows  burgundies  purples  browns  multi  \\\n",
       "0     0  ...      0      0     0        0           0        0       0      0   \n",
       "1     0  ...      0      0     0        0           0        0       0      0   \n",
       "2     0  ...      0      0     0        0           0        0       0      0   \n",
       "3     0  ...      0      0     0        0           0        0       0      0   \n",
       "4     0  ...      0      0     0        0           0        0       1      0   \n",
       "\n",
       "   oranges  teal  \n",
       "0        0     0  \n",
       "1        0     0  \n",
       "2        0     0  \n",
       "3        0     0  \n",
       "4        0     0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_color.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_color = df_color.brand + df_color.product_full_name + df_color.description + df_color.brand_category + df_color.details\n",
    "doc_fit = df_fit.brand + df_fit .product_full_name + df_fit.description + df_fit.brand_category + df_fit.details\n",
    "doc_occasion = df_occasion.brand + df_occasion.product_full_name + df_occasion.description + df_occasion.brand_category + df_occasion.details\n",
    "doc_style = df_style.brand + df_style.product_full_name + df_style.description + df_style.brand_category + df_style.details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count_Vectorizer with Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-186831e47d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mcolumns_style\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'semi-fitted'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'relaxed'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'straight / regular'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fitted / tailored'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'oversized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mmodel_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_color\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_color\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns_color\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;31m#model_fit = logistic_model(doc_fit,df_fit,columns_fit)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#model_occasion = logistic_model(doc_occasion,df_occasion,columns_occasion)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-186831e47d5b>\u001b[0m in \u001b[0;36mlogistic_model\u001b[1;34m(doc, df, columns)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlogistic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#X = vectorizer.transform(X_test) juse for test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_name' is not defined"
     ]
    }
   ],
   "source": [
    "# 42 models trained\n",
    "\n",
    "def logistic_model(doc,df,columns):\n",
    "    \n",
    "    vectorizer = CountVectorizer(feature_name)\n",
    "    X = vectorizer.fit(doc)\n",
    "    #X = vectorizer.transform(X_test) juse for test\n",
    "    X = X.toarray()\n",
    "    X = StandardScaler().fit_transform(X)# same for this (separtely)\n",
    "    data = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for col in columns:\n",
    "        y = df[col].values\n",
    "        #base_accuracy = y.sum()/len(y)\n",
    "        #base_accuracy = max(base_accuracy,1-base_accuracy)\n",
    "\n",
    "        data[\"TARGET\"] = y\n",
    "\n",
    "        train_df, test_df = train_test_split(data)\n",
    "        X_train = train_df.loc[:, ~train_df.columns.isin(['TARGET'])]\n",
    "        X_test = test_df.loc[:, ~test_df.columns.isin(['TARGET'])]\n",
    "\n",
    "        y_train = train_df[\"TARGET\"]\n",
    "        y_test = test_df[\"TARGET\"]\n",
    "\n",
    "        clf =linear_model.LogisticRegression(C=0.001,random_state=None).fit(X_train, y_train)\n",
    "           \n",
    "        #models.append[clf]\n",
    "        #y_pred = clf.predict(X_test)\n",
    "\n",
    "        #acc = np.mean(y_pred == y_test)\n",
    "        models.append(clf)\n",
    "    return X_test\n",
    "\n",
    "\n",
    "columns_color = ['blacks','pinks','whites','reds','greens','blues','silvers','neutrals','oranges',\n",
    "                'beiges','grays','golds','navy','yellows','burgundies','purples','browns','multi','teal']\n",
    "columns_fit   = ['business casual','classic','modern','boho','glam','romantic','casual','androgynous','edgy','retro','athleisure']\n",
    "columns_occasion = ['day to night','work','weekend','night out','vacation','coldweather','workout'] \n",
    "columns_style = ['semi-fitted','relaxed','straight / regular','fitted / tailored','oversized']\n",
    "\n",
    "model_color = logistic_model(doc_color,df_color,columns_color)\n",
    "#model_fit = logistic_model(doc_fit,df_fit,columns_fit)\n",
    "#model_occasion = logistic_model(doc_occasion,df_occasion,columns_occasion)\n",
    "#model_style = logistic_model(doc_style,df_style,columns_style)\n",
    "\n",
    "#model_list = model_color+model_fit+model_occasion+model_style\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_color' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0f330dda1959>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_color\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_color' is not defined"
     ]
    }
   ],
   "source": [
    "model_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = ['blacks','pinks','whites','reds','greens','blues','silvers','neutrals','oranges',\n",
    "                'beiges','grays','golds','navy','yellows','burgundies','purples','browns','multi','teal'\n",
    "'business casual','classic','modern','boho','glam','romantic','casual','androgynous','edgy','retro','athleisure'\n",
    "'day to night','work','weekend','night out','vacation','coldweather','workout'\n",
    "'semi-fitted','relaxed','straight / regular','fitted / tailored','oversized']\n",
    "\n",
    "dftest = pd.DataFrame(ylabel,columns = [\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21</th>\n",
       "      <th>34</th>\n",
       "      <th>blue</th>\n",
       "      <th>color</th>\n",
       "      <th>denim</th>\n",
       "      <th>forever</th>\n",
       "      <th>jeans</th>\n",
       "      <th>size</th>\n",
       "      <th>slim</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>1.825742</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>1.825742</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>1.825742</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.547723</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          21        34      blue     color     denim   forever     jeans  \\\n",
       "0  -0.288675 -0.288675 -0.288675 -0.288675 -0.288675  3.464102 -0.547723   \n",
       "1   3.464102 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.547723   \n",
       "2  -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675  1.825742   \n",
       "3  -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.547723   \n",
       "4  -0.288675  3.464102 -0.288675 -0.288675 -0.288675 -0.288675 -0.547723   \n",
       "5  -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.547723   \n",
       "6  -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.547723   \n",
       "7  -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.547723   \n",
       "8  -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675  1.825742   \n",
       "9  -0.288675 -0.288675 -0.288675 -0.288675  3.464102 -0.288675 -0.547723   \n",
       "10 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675 -0.288675  1.825742   \n",
       "11 -0.288675 -0.288675  3.464102 -0.288675 -0.288675 -0.288675 -0.547723   \n",
       "12 -0.288675 -0.288675 -0.288675  3.464102 -0.288675 -0.288675 -0.547723   \n",
       "\n",
       "        size      slim      this  \n",
       "0  -0.288675 -0.288675 -0.288675  \n",
       "1  -0.288675 -0.288675 -0.288675  \n",
       "2  -0.288675 -0.288675 -0.288675  \n",
       "3   3.464102 -0.288675 -0.288675  \n",
       "4  -0.288675 -0.288675 -0.288675  \n",
       "5  -0.288675 -0.288675 -0.288675  \n",
       "6  -0.288675 -0.288675  3.464102  \n",
       "7  -0.288675  3.464102 -0.288675  \n",
       "8  -0.288675 -0.288675 -0.288675  \n",
       "9  -0.288675 -0.288675 -0.288675  \n",
       "10 -0.288675 -0.288675 -0.288675  \n",
       "11 -0.288675 -0.288675 -0.288675  \n",
       "12 -0.288675 -0.288675 -0.288675  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_brand = \"Forever 21\"\n",
    "test_product_full_name = \"Jeans size 34 M,\"\n",
    "test_description = \"This is a slim jeans\"\n",
    "test_brand_category = \"Denim Jeans\"\n",
    "test_details = \"Blue color\"\n",
    "\n",
    "test_docs = test_brand +\" \" + test_product_full_name + \" \" + test_description + \" \" + test_brand_category +  \" \" + test_details\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "# Remove Punctuations\n",
    "punctuation = \"!@#$%^&*()_+<>?:.,;\"  \n",
    "    \n",
    "for c in test_docs:\n",
    "    if c in punctuation:\n",
    "        test_docs = test_docs.replace(c, \"\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Remove Stopwords  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(test_docs) \n",
    "test_docs = [w for w in word_tokens if not w in stop_words] \n",
    "test_docs = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        test_docs.append(w) \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(test_docs) \n",
    "X = X.toarray()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "test_data = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "\n",
    "test_data\n",
    "#y_pred = []\n",
    "#model_list[0]\n",
    "#model_list[0].predict(test_data)\n",
    "    \n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAX_SEQUENCE_LENGTH(list1):\n",
    "    max = 0\n",
    "    for i in list1:\n",
    "        if max<len(i):\n",
    "            max=len(i)\n",
    "    return max\n",
    "\n",
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def make_binary_classification_rnn_model(plot=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def make_lstm_classification_model(plot = False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Glove + LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(list(doc_color))\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(doc_color, tokenizer)\n",
    "\n",
    "# padding to create equal length sequences\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "padded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(df_color['blacks']))\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2)\n",
    "\n",
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)\n",
    "\n",
    "# Load in GloVe Vectors\n",
    "embeddings_index = load_glove_vectors()\n",
    "embeddings_index\n",
    "\n",
    "# # create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# define model\n",
    "model = make_lstm_classification_model()\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = [\n",
    "    \"Employees look like they hate their job. Milkshake was like drinking milk. Food was cold and not warm at all\",\n",
    "    \"This Mcdonalds is not only in the business of making crappy food and providing even crappier service watch out for the racket they have in the parking lot . If your not careful reading the sign at the the front of the entrance it is going to cost you $195.00 in parking fees. went in to to ask the management they just blew me off. lucky they are in vegas where they dont count on repeat businesssss.\",\n",
    "    \"There are better stores without fruit flies in Griffin, GA.\",\n",
    "    \"Slowest drive-thru ever. Better option is to go to the location on arlington\"\n",
    "]\n",
    "\n",
    "test_docs = list(\n",
    "    map(lambda doc: \" \".join([token.text for token in nlp(doc) if not token.is_stop]), test_docs))\n",
    "\n",
    "encoded_test_sample = integer_encode_documents(test_docs, tokenizer)\n",
    "\n",
    "padded_test_docs = keras.preprocessing.sequence.pad_sequences(encoded_test_sample, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "model.predict_classes(padded_test_docs)\n",
    "prediction = model.predict_classes(padded_test_docs)\n",
    "encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word2Vec (Equal Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(list(doc_color))\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(doc_color, tokenizer)\n",
    "\n",
    "# padding to create equal length sequences\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "padded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(df_color['blacks']))\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2)\n",
    "\n",
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)\n",
    "\n",
    "# Load in GloVe Vectors\n",
    "embedding_matrix = []\n",
    "for i in doc_color:\n",
    "   embedding_matrix.append(nlp(i).vector)\n",
    "embedding_matrix = np.asarray(embedding_matrix)\n",
    "# embeddings_index = load_glove_vectors()\n",
    "# embeddings_index\n",
    "\n",
    "#create a weight matrix for words in training docs\n",
    "# embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# define model\n",
    "model = make_lstm_classification_model2()\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(doc).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"black shoes green belt\"\n",
    "# nlp(test).vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GLOVE (Unequal Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(doc)\n",
    "# X = X.toarray()\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Self Trained Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = list(doc.values)\n",
    "# doc = [word_tokenize(review) for review in doc]\n",
    "# model = Word2Vec(doc, min_count=5)\n",
    "# words = list(model.wv.vocab)\n",
    "# vectors = []\n",
    "# for word in words:\n",
    "#     vectors.append(model[word].tolist())\n",
    "# data = np.array(vectors)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using count vectorization to find out more words that lemmatization could not remove and assigning them to base form for the purpose of dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting remaning unchanged words to their base form manually\n",
    "# doc1 = re.sub(r'wearability|wearable|wearin|wearing','wear',doc1)\n",
    "# doc1 = re.sub(r'transitioning|transitioned|transitional','transition',doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_style.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count vectorization for full data\n",
    "\n",
    "# # Subset of the broader category\n",
    "\n",
    "\n",
    "\n",
    "# doc = df_occasion.brand + df_occasion.product_full_name + df_occasion.description + df_occasion.brand_category + df_occasion.details\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(doc) \n",
    "# X = X.toarray()\n",
    "# columns = ['day to night','work','weekend','night out','vacation','coldweather','workout']\n",
    "\n",
    "\n",
    "# data = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# accuracy = []\n",
    "# for col in columns:\n",
    "#     y = df_occasion[col].values\n",
    "#     base_accuracy = y.sum()/len(y)\n",
    "#     base_accuracy = max(base_accuracy,1-base_accuracy)\n",
    "    \n",
    "#     data[\"TARGET\"] = y\n",
    "\n",
    "#     train_df, test_df = train_test_split(data)\n",
    "#     X_train = train_df.loc[:, ~train_df.columns.isin(['TARGET'])]\n",
    "#     X_test = test_df.loc[:, ~test_df.columns.isin(['TARGET'])]\n",
    "\n",
    "#     y_train = train_df[\"TARGET\"]\n",
    "#     y_test = test_df[\"TARGET\"]\n",
    "\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     y_pred = lr.predict(X_test)\n",
    "\n",
    "#     acc = np.mean(y_pred == y_test)\n",
    "#     accuracy.append([col,acc,base_accuracy])\n",
    "# accuracy\n",
    "# # X= X.toarray()\n",
    "# # countVector = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "# # pd.set_option('display.max_columns', None)\n",
    "# # countVector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_color.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = df_color.brand + df_color.product_full_name + df_color.description + df_color.brand_category + df_color.details\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(doc) \n",
    "# X = X.toarray()\n",
    "# columns = ['blacks',\n",
    "#  'pinks',\n",
    "#  'whites',\n",
    "#  'reds',\n",
    "#  'greens',\n",
    "#  'blues',\n",
    "#  'silvers',\n",
    "#  'neutrals',\n",
    "#  'beiges',\n",
    "#  'grays',\n",
    "#  'golds',\n",
    "#  'navy',\n",
    "#  'yellows',\n",
    "#  'burgundies',\n",
    "#  'purples',\n",
    "#  'browns',\n",
    "#  'multi',\n",
    "#  'oranges',\n",
    "#  'teal']\n",
    "\n",
    "\n",
    "# data = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "# accuracy = []\n",
    "# for col in columns:\n",
    "#     y = df_color[col].values\n",
    "#     base_accuracy = y.sum()/len(y)\n",
    "#     base_accuracy = max(base_accuracy,1-base_accuracy)\n",
    "\n",
    "    \n",
    "#     data[\"TARGET\"] = y\n",
    "\n",
    "#     train_df, test_df = train_test_split(data)\n",
    "#     X_train = train_df.loc[:, ~train_df.columns.isin(['TARGET'])]\n",
    "#     X_test = test_df.loc[:, ~test_df.columns.isin(['TARGET'])]\n",
    "\n",
    "#     y_train = train_df[\"TARGET\"]\n",
    "#     y_test = test_df[\"TARGET\"]\n",
    "\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     y_pred = lr.predict(X_test)\n",
    "\n",
    "#     acc = np.mean(y_pred == y_test)\n",
    "#     accuracy.append([col,acc,base_accuracy])\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = df_fit.brand + df_fit.product_full_name + df_fit.description + df_fit.brand_category + df_fit.details\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(doc) \n",
    "# X = X.toarray()\n",
    "# columns = ['business casual',\n",
    "#  'classic',\n",
    "#  'modern',\n",
    "#  'boho',\n",
    "#  'glam',\n",
    "#  'romantic',\n",
    "#  'casual',\n",
    "#  'androgynous',\n",
    "#  'edgy',\n",
    "#  'retro',\n",
    "#  'athleisure']\n",
    "\n",
    "\n",
    "# data = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "# accuracy = []\n",
    "# for col in columns:\n",
    "#     y = df_fit[col].values\n",
    "#     base_accuracy = y.sum()/len(y)\n",
    "#     base_accuracy = max(base_accuracy,1-base_accuracy)\n",
    "    \n",
    "#     data[\"TARGET\"] = y\n",
    "\n",
    "#     train_df, test_df = train_test_split(data)\n",
    "#     X_train = train_df.loc[:, ~train_df.columns.isin(['TARGET'])]\n",
    "#     X_test = test_df.loc[:, ~test_df.columns.isin(['TARGET'])]\n",
    "\n",
    "#     y_train = train_df[\"TARGET\"]\n",
    "#     y_test = test_df[\"TARGET\"]\n",
    "\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     y_pred = lr.predict(X_test)\n",
    "\n",
    "#     acc = np.mean(y_pred == y_test)\n",
    "#     accuracy.append([col,acc,base_accuracy])\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = df_style.brand + df_style.product_full_name + df_style.description + df_style.brand_category + df_style.details\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(doc) \n",
    "# X = X.toarray()\n",
    "# columns = [\n",
    "#  'semi-fitted',\n",
    "#  'relaxed',\n",
    "#  'straight / regular',\n",
    "#  'fitted / tailored',\n",
    "#  'oversized']\n",
    "\n",
    "\n",
    "# data = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "# accuracy = []\n",
    "# for col in columns:\n",
    "#     y = df_style[col].values\n",
    "#     base_accuracy = y.sum()/len(y)\n",
    "#     base_accuracy = max(base_accuracy,1-base_accuracy)\n",
    "#     data[\"TARGET\"] = y\n",
    "\n",
    "#     train_df, test_df = train_test_split(data)\n",
    "#     X_train = train_df.loc[:, ~train_df.columns.isin(['TARGET'])]\n",
    "#     X_test = test_df.loc[:, ~test_df.columns.isin(['TARGET'])]\n",
    "\n",
    "#     y_train = train_df[\"TARGET\"]\n",
    "#     y_test = test_df[\"TARGET\"]\n",
    "    \n",
    "#     clf =linear_model.LogisticRegression(C=0.001,random_state=None)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = lr.predict(X_test)\n",
    "\n",
    "#     acc = np.mean(y_pred == y_test)\n",
    "#     accuracy.append([col,acc,base_accuracy])\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = df_occasion.brand + df_occasion.product_full_name + df_occasion.description + df_occasion.brand_category + df_occasion.details\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(doc) \n",
    "# X = X.toarray()\n",
    "# columns = ['day to night','work','weekend','night out','vacation','coldweather','workout']\n",
    "\n",
    "\n",
    "# data = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# n=[100,200,300]\n",
    "# max_depth=[2,4,6,8]\n",
    "# for i in n:\n",
    "#     for j in max_depth:\n",
    "#         accuracy = []\n",
    "#         for col in columns:\n",
    "#             y = df_occasion[col].values\n",
    "#             base_accuracy = y.sum()/len(y)\n",
    "#             base_accuracy = max(base_accuracy,1-base_accuracy)\n",
    "\n",
    "#             data[\"TARGET\"] = y\n",
    "\n",
    "#             train_df, test_df = train_test_split(data)\n",
    "#             X_train = train_df.loc[:, ~train_df.columns.isin(['TARGET'])]\n",
    "#             X_test = test_df.loc[:, ~test_df.columns.isin(['TARGET'])]\n",
    "\n",
    "#             y_train = train_df[\"TARGET\"]\n",
    "#             y_test = test_df[\"TARGET\"]\n",
    "\n",
    "#             rf = RandomForestClassifier(max_depth=j, n_estimators = i,  n_jobs = -1,max_features = 10).fit(X_train, y_train)\n",
    "#             y_pred = rf.predict(X_test)\n",
    "\n",
    "#             acc = np.mean(y_pred == y_test)\n",
    "#             accuracy.append([col,acc,base_accuracy])\n",
    "#         print(f\"max_depth: {j}, estimators: {i}\\n{accuracy}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boosted Trees\n",
    "# doc = df_occasion.brand + df_occasion.product_full_name + df_occasion.description + df_occasion.brand_category + df_occasion.details\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(doc) \n",
    "# X = X.toarray()\n",
    "# columns = ['day to night','work','weekend','night out','vacation','coldweather','workout']\n",
    "\n",
    "\n",
    "# data = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# n=[100,200,300]\n",
    "# max_depth=[3,4]\n",
    "# for i in n:\n",
    "#     for j in max_depth:\n",
    "#             accuracy = []\n",
    "#             for col in columns:\n",
    "#                 y = df_occasion[col].values\n",
    "#                 base_accuracy = y.sum()/len(y)\n",
    "#                 base_accuracy = max(base_accuracy,1-base_accuracy)\n",
    "\n",
    "#                 data[\"TARGET\"] = y\n",
    "\n",
    "#                 train_df, test_df = train_test_split(data)\n",
    "#                 X_train = train_df.loc[:, ~train_df.columns.isin(['TARGET'])]\n",
    "#                 X_test = test_df.loc[:, ~test_df.columns.isin(['TARGET'])]\n",
    "\n",
    "#                 y_train = train_df[\"TARGET\"]\n",
    "#                 y_test = test_df[\"TARGET\"]\n",
    "\n",
    "#                 bt=GradientBoostingClassifier(n_estimators=i, learning_rate=0.1,max_depth=j).fit(X_train, y_train)\n",
    "#                 y_pred = bt.predict(X_test)\n",
    "\n",
    "#                 acc = np.mean(y_pred == y_test)\n",
    "#                 accuracy.append([col,acc,base_accuracy])\n",
    "#             print(f\"max_depth: {j}, estimators: {i}\\n{accuracy}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TF-IDF Weighted Average Word Embeddings\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(doc1)\n",
    "# X = X.toarray()\n",
    "# tf_idf = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# # sum the tf idf scores for each document\n",
    "# tf_idf[\"TF_IDF_SUM\"] = tf_idf.sum(axis=1)\n",
    "# tf_idf_scores = list(map( lambda x: x.lower(), tf_idf.columns))\n",
    "# tf_idf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING SECTION FOR EVALUATION (FOR PROFESSOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_brand = \"Forever 21\"\n",
    "test_product_full_name = \"Jeans size 34 M\"\n",
    "test_description = \"This is a slim jeans\"\n",
    "test_brand_category = \"Denim Jeans\"\n",
    "test_details = \"Blue color\"\n",
    "\n",
    "\n",
    "\n",
    "# test_brand = str(input(\"Enter Brand: \"))\n",
    "# test_product_full_name = str(input(\"Product_Full_Name: \"))\n",
    "# test_description = str(input(\"Product Description: \"))\n",
    "# test_brand_category = str(input(\"Brand Category: \"))\n",
    "# test_details = str(input(\"Details: \"))\n",
    "\n",
    "# MAX_SEQUENCE_LENGTH = 4\n",
    "\n",
    "# test_docs = test_brand +\" \" + test_product_full_name + \" \" + test_description + \" \" + test_brand_category +  \" \" + test_details\n",
    "# test_docs = list(map(lambda doc: \" \".join([token.text for token in nlp(doc) if not token.is_stop]), test_docs))\n",
    "\n",
    "# encoded_test_sample = integer_encode_documents(test_docs, tokenizer)\n",
    "# padded_test_docs = keras.preprocessing.sequence.pad_sequences(encoded_test_sample, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "\n",
    "# VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)\n",
    "\n",
    "\n",
    "\n",
    "# from keras.layers.recurrent import SimpleRNN\n",
    "# from keras.layers import Flatten, Masking\n",
    "\n",
    "\n",
    "# def load_glove_vectors():\n",
    "#     embeddings_index = {}\n",
    "     \n",
    "#     with open('glove.6B.100d.txt',encoding = \"utf8\") as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "#     print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "#     return embeddings_index\n",
    "\n",
    "# labels = 1\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder = LabelEncoder()\n",
    "# #labels = to_categorical(encoder.fit_transform(labels))\n",
    "\n",
    "# embeddings_index = load_glove_vectors()\n",
    "\n",
    "# embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "# embedding_matrix\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "# model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "# model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "# model.add(Dense(32))\n",
    "# model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# prediction = model.predict_classes(padded_test_docs)\n",
    "# encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occasion_vectors = []\n",
    "# for idx, occasion in enumerate(occasions): # iterate through each document\n",
    "#     tokens = nlp(occasion) # have spacy tokenize the review text\n",
    "    \n",
    "#     # initially start a running total of tf-idf scores for a document\n",
    "#     total_tf_idf_score_per_document = 0\n",
    "    \n",
    "#     # start a running total of initially all zeroes (300 is picked since that is the word embedding size used by word2vec)\n",
    "#     running_total_word_embedding = np.zeros(300) \n",
    "#     for token in tokens: # iterate through each token\n",
    "    \n",
    "#     # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "#         if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "            \n",
    "#             tf_idf_score = tf_idf_lookup_table.loc[idx, token.text.lower()]\n",
    "#             #print(f\"{token} has tf-idf score of {tf_idf_lookup_table.loc[idx, token.text.lower()]}\")\n",
    "#             running_total_word_embedding += tf_idf_score * token.vector\n",
    "            \n",
    "#             total_tf_idf_score_per_document += tf_idf_score\n",
    "    \n",
    "#     # divide the total embedding by the total tf-idf score for each document\n",
    "#     document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "#     occasion_vectors.append(document_embedding)\n",
    "# occasion_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
